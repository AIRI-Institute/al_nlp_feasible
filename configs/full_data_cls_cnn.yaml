output_dir: './workdir/run_active_learning'
hydra:
  run:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}_${seed}_${to_string:${model.name}}
  sweep:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}_${seed}_${to_string:${model.name}}
    subdir: ${hydra.job.num}

seed: 42
cuda_device: 0
cache_dir: ./${output_dir}/cache_${seed}_${to_string:${model.name}}
cache_model_and_dataset: True
dump_model: True
framework: ???
task: 'cls'
offline_mode: False

data:
    dataset_name: 'ag_news'
    text_name: 'text'
    label_name: 'label'
    labels_to_remove:
    path: 'datasets'
    train_size_split: 0.9

model:
    embeddings_path: 'word2vec' # add cnn-specific params
    embeddings_cache_dir: './workdir/run_active_learning/embeddings'
    freeze_embedding: False
    vocab_size: None
    embed_dim: 300
    filter_sizes: [3,4,5]
    num_filters: [100,100,100]
    type: ${task}
    name: 'cnn'
    tokenizer_max_length: 256
    num_labels:
    classifier_dropout: 0.5
    exists_in_repo: False # whether the model exists in HF models repo
    path_to_pretrained: # required if the model does not exist in HF models repo
    training:
        dev_size: 0.1
        shuffle_dev: False
        freeze_embedder: False
        batch_size_args:
            batch_size: 128
            eval_batch_size: 100
            min_num_gradient_steps: 350
            adjust_batch_size: True
            adjust_num_epochs: True
            min_batch_size: 128
        trainer_args:
            num_epochs: 20
            patience: 5
            grad_clipping: 1.
            serialization_dir: ./output/${to_string:${model.name}}_${seed}_${now:%Y-%m-%d}_${now:%H-%M-%S}
            validation_metric: ${framework.validation_metric_cls}
            evaluation_strategy: 'epoch' # for transformers wrapper, 'epoch' or 'steps' or 'no'
            load_best_at_end: True
            eval_metrics: ["f1"]
            fp16:
                training: True
                evaluation: False
            accumulation:
                gradient_accumulation_steps: 1
                eval_accumulation_steps:
        optimizer_args:
            weight_decay: 0.0
            lr: 0.001
        scheduler_args:
            warmup_steps_factor: 0.0
            use_adafactor: False

defaults:
  - framework: transformers # 'allennlp' or 'transformers'